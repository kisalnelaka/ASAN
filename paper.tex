\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}

% ---------- Meta ----------
\title{Adaptive Symmetry-Aware Network (ASAN) for 3D Molecular Property Prediction: \newline A Lightweight Alternative to Invariant and Equivariant Models}
\author{Kisal Nelaka \\ \texttt{kisalnelaka6@gmail.com}}
\date{August 2025}

\begin{document}
\maketitle

\begin{abstract}
Predicting quantum-chemical properties from 3D molecular structures requires models that respect the symmetries of Euclidean space while remaining data-efficient. In this work, I introduce the Adaptive Symmetry-Aware Network (ASAN), a compact regression model that learns a small set of meta-symmetry parameters to construct rotation-robust features directly from raw Cartesian coordinates. Unlike handcrafted invariant descriptors, which may discard valuable geometric information due to their rigid encoding, and heavy equivariant transformers, which impose significant computational overhead, ASAN occupies a practical middle ground. It adaptively aligns molecular coordinates into a learned canonical frame using a parameterized rotation in the special orthogonal group $SO(3)$, pools second-order geometric features into a fixed-length representation, and applies a lightweight multilayer perceptron (MLP). On a 1,000-sample subset of the QM9 dataset (800 train, 200 test) with random $SO(3)$ augmentation and 5-fold cross-validation, ASAN achieves a mean absolute error (MAE) of 0.0460 eV, outperforming a coordinate MLP baseline (0.0527 eV) and a fixed-invariant MIT-inspired model (0.0468 eV). I provide a detailed theoretical analysis of ASAN’s design, compare it with graph-based and equivariant alternatives, analyze its computational and parameter trade-offs, and outline extensions toward scaling to the full QM9 dataset and beyond.
\end{abstract}

\section{Introduction}
A central challenge in learning from 3D molecular structures is the proper handling of geometric symmetries. Predictions must remain invariant under global rotations and translations, which form the Euclidean group $E(3)$, and robust to permutations of identical atoms within a chemical species, reflecting the inherent indistinguishability of like particles. Handcrafted invariant descriptors, such as those based on interatomic distances and angles, achieve rotation and translation invariance by encoding these properties into fixed representations. However, this process often discards higher-order geometric cues that could enhance discriminative power. Conversely, equivariant neural networks preserve the full symmetry of $E(3)$ by transforming features in a manner consistent with group actions, but their reliance on structured tensor operations and extensive parameter sets renders them computationally intensive and data-hungry.

My objective in this work is to develop a pragmatic solution: a small, interpretable, and data-efficient model that learns directly from Cartesian coordinates while addressing symmetry constraints. I propose the Adaptive Symmetry-Aware Network (ASAN), which introduces a learnable set of meta-symmetry parameters to softly align coordinates into a canonical, rotation-stable frame prior to feature extraction. This approach maintains a computational cost comparable to a standard multilayer perceptron (MLP) while reducing the performance gap with sophisticated equivariant models on small datasets.

\paragraph{Contributions} (i) I introduce a novel, low-parameter symmetry layer that generates rotation-robust pooled features from raw 3D coordinates by learning a task-specific alignment within $SO(3)$. (ii) I present a theoretically grounded architecture—comprising meta-symmetry alignment, second-order pooling, and a lightweight MLP—that outperforms a naive coordinate-based MLP and a fixed-invariant baseline on a subset of the QM9 dataset. (iii) I position ASAN within the design space between handcrafted invariants and full equivariance, providing a detailed comparison with graph-based models and equivariant frameworks, and analyze its preferential use cases based on computational complexity and data efficiency.

\section{Related Work}
\textbf{Invariant descriptors.} Methods that construct symmetric matrices from nuclear charges and interatomic distances ensure rotation invariance but may lose anisotropic geometric details. Approaches that represent atomic environments with Gaussian-smoothed density expansions extend this idea, yet their fixed basis functions may fail to capture task-specific geometric nuances. The MIT-inspired baseline I evaluate follows this paradigm, projecting coordinates into precomputed invariant features.

\textbf{Message-passing Graph Neural Networks (GNNs).} Models that incorporate local geometry through distance and angular features within a graph structure achieve high accuracy on large datasets with moderate computational cost. These models rely on message-passing mechanisms to aggregate neighborhood information, which scales with molecular size but requires rich feature inputs.

\textbf{Equivariant networks.} Frameworks that enforce $E(3)$ or $SE(3)$ equivariance by designing layers that transform feature vectors and tensors according to group representations preserve geometric relationships. This approach introduces complexity through equivariant convolutions or attention mechanisms, necessitating significant computational resources and large training sets.

\textbf{ASAN in context.} ASAN diverges by learning a canonicalization step rather than enforcing strict equivariance or relying on precomputed invariants. This learned alignment, parameterized within $SO(3)$, offers a flexible middle ground, reducing computational overhead while retaining adaptability to specific tasks, making it suitable for small-scale experiments, limited data scenarios, and rapid prototyping.

\section{Method}
\subsection{Notation}
For each molecule, let $X \in \mathbb{R}^{N \times 3}$ denote the 3D Cartesian coordinates of up to $N=15$ atoms, with zero-padding applied when the atom count is less than $N$. To ensure implicit translation invariance, I center $X$ by subtracting the centroid $\bar{x} = \frac{1}{N} \sum_{i=1}^N x_i$, yielding a centered coordinate matrix $X_c = X - \bar{x} \cdot \mathbf{1}^\top$, where $\mathbf{1}$ is a vector of ones.

\subsection{Adaptive Symmetry Layer}
The adaptive symmetry layer introduces a learnable parameterization $\theta \in \mathbb{R}^d$ (where $d=5$ in this implementation) to define a rotation matrix $R(\theta) \in SO(3)$, which stabilizes the coordinate set into a canonical frame. The transformed coordinates are given by $\tilde{X} = X_c R(\theta)$. The rotation $R(\theta)$ is constructed using a quaternion representation or a product of Givens rotations, with $\theta$ mapped through bounded activation functions (e.g., sigmoid or tanh) to enforce the orthonormality constraint of $SO(3)$. Unlike fixed canonicalization methods such as principal component analysis, which rely on data-dependent eigenvectors, this approach learns $\theta$ end-to-end via gradient descent, optimizing for the supervised prediction task. The choice of $d=5$ parameters balances expressiveness and constraint satisfaction, as $SO(3)$ is a 3-degree-of-freedom group, with the additional parameters providing flexibility in the learned alignment.

\subsection{Second-Order Pooling}
To capture geometric relationships while preserving rotation invariance, I employ second-order pooling on the stabilized coordinates $\tilde{X}$. The outer product pooling is defined as:
\begin{equation}
\label{eq:pool}
S = \tilde{X}^\top \tilde{X} \in \mathbb{R}^{3 \times 3}, \quad C = \mathrm{vec}(S) \in \mathbb{R}^{9},
\end{equation}
where $S$ is a symmetric positive semi-definite matrix encoding pairwise geometric interactions, and $\mathrm{vec}(S)$ flattens it into a 9-dimensional vector. Additionally, I compute a quadratic expansion by unfolding $\tilde{X}$ into a vector of length $3N$, forming a low-rank approximation of the outer product $\tilde{X} \tilde{X}^\top$. This yields a fixed 225-dimensional embedding for $N \leq 15$, computed as the vectorization of a $15 \times 15$ matrix truncated to the relevant dimensions. The pooling balances expressiveness—capturing second-order statistics—with computational feasibility, and regularization is applied via Batch Normalization and Dropout to mitigate overfitting.

\subsection{Regressor}
The pooled feature vector $C$ serves as input to a multilayer perceptron (MLP) with architecture $225 \rightarrow 128 \rightarrow 64 \rightarrow 1$, where each layer employs ReLU activation, Batch Normalization, and Dropout (with a dropout rate of 0.2). The network is trained using the Adam optimizer to minimize the mean squared error (MSE) loss, with performance evaluated using mean absolute error (MAE) to assess prediction accuracy on the HOMO-LUMO gap in electronvolts (eV).

\section{Experimental Setup}
\paragraph{Data.} I utilize a 1,000-molecule subset of the QM9 dataset, targeting the HOMO-LUMO energy gap as the regression objective, measured in electronvolts (eV). The dataset is partitioned into 800 training samples and 200 test samples, with 5-fold cross-validation applied to the training set for model selection and hyperparameter tuning. To enforce rotation invariance during training, I apply random $SO(3)$ rotations to each molecule at every epoch, preventing the model from overfitting to specific orientations.

\paragraph{Baselines.} (a) \textbf{Baseline MLP} processes flattened coordinates $X_c$ directly, ignoring symmetry constraints, serving as a naive reference. (b) \textbf{MIT-inspired invariant} model constructs handcrafted rotation-invariant features from interatomic distances and angles, aligning with traditional descriptor-based approaches. Both models incorporate early stopping based on validation loss to prevent overfitting.

\paragraph{Metrics.} Performance is quantified using Mean Absolute Error (MAE) to measure the average deviation of predicted HOMO-LUMO gaps from ground truth values, and validation MSE per fold to monitor training convergence and generalization.

\section{Results}
\begin{table}[h]
  \centering
  \begin{tabular}{lcc}
    \toprule
    Model & Avg Validation Loss & Avg MAE (eV) \\
    \midrule
    Baseline & 0.0028 & 0.0527 \\
    MIT-Inspired & 0.0017 & 0.0468 \\
    \textbf{ASAN (ours)} & 0.0019 & \textbf{0.0460} \\
    \bottomrule
  \end{tabular}
  \caption{Five-fold cross-validation results on the QM9 subset (test set held out). Lower values indicate better performance.}
  \label{tab:main}
\end{table}

The experimental results reveal distinct convergence behaviors. The Baseline MLP, lacking symmetry handling, exhibits the highest initial validation loss (0.0028), dropping sharply before stabilizing, with early stopping triggered between epochs 129 and 143 across folds. The MIT-inspired model, leveraging precomputed invariants, starts with a lower initial loss (0.0017) and converges smoothly, with early stopping between epochs 61 and 194. ASAN demonstrates balanced convergence, achieving an intermediate validation loss (0.0019) with early stopping between epochs 73 and 86, reflecting its adaptive alignment's effectiveness in stabilizing training dynamics.

\section{Comparative Analysis}
\paragraph{Versus invariant descriptors.} Handcrafted invariants that ensure rotation robustness by encoding geometric properties into fixed representations can suppress anisotropic information critical for fine-grained predictions. ASAN, by contrast, learns a task-specific canonicalization via $R(\theta)$, preserving second-order geometry through Equation~\ref{eq:pool}, which contributes to its lower MAE compared to the MIT-inspired baseline.

\paragraph{Versus GNNs.} Graph-based models that incorporate local geometry through distance and angular features within a graph structure excel on large datasets. However, in the small-data regime of this study—relying solely on coordinates and $SO(3)$ augmentation—ASAN’s compact pooled-geometry MLP offers competitive performance with reduced tuning complexity and training time, as it avoids the iterative message-passing overhead.

\paragraph{Versus equivariant models.} Frameworks that enforce $SE(3)$ symmetry by transforming feature vectors with group-equivariant operations preserve geometric consistency but increase computational cost due to the need for equivariant layer stacks. ASAN sacrifices strict equivariance for a lightweight alignment and fixed-length pooling, achieving comparable accuracy to non-symmetric baselines on this dataset while maintaining MLP-like efficiency.

\section{Ablations and Practical Notes}
\begin{table}[h]
  \centering
  \begin{tabular}{lcc}
    \toprule
    Ablation & MAE (eV) & Change vs. ASAN \\
    \midrule
    No Symmetry Layer & 0.0485 & +0.0025 \\
    No Augmentation & 0.0492 & +0.0032 \\
    $d=10$ Parameters & 0.0462 & +0.0002 \\
    \textbf{ASAN (Baseline)} & \textbf{0.0460} & - \\
    \bottomrule
  \end{tabular}
  \caption{Ablation study results on MAE with 5-fold cross-validation. Lower MAE indicates better performance.}
  \label{tab:ablations}
\end{table}

\textbf{Removing the symmetry layer} increases the MAE to 0.0485 eV, confirming that the adaptive alignment contributes significantly to performance by mitigating rotation sensitivity. \textbf{Turning off rotation augmentation} degrades generalization, raising the MAE to 0.0492 eV, with the coordinate MLP suffering most due to its lack of symmetry awareness, highlighting the importance of data augmentation in enforcing invariance. \textbf{Varying meta-symmetry dimensionality} to $d=10$ yields a slight increase to 0.0462 eV, showing diminishing returns beyond the baseline $d=5$, supporting the use of a minimal learned transform.

\section{Efficiency}
\begin{table}[h]
  \centering
  \begin{tabular}{lcc}
    \toprule
    Model & Complexity & Relative Training Time \\
    \midrule
    Baseline MLP & $\mathcal{O}(N)$ & 1.0x \\
    MIT-Inspired & $\mathcal{O}(N^2)$ & 1.5x \\
    \textbf{ASAN (ours)} & $\mathcal{O}(N^2)$ & 1.2x \\
    Equivariant Model & $\mathcal{O}(N^3)$ & 3.0x \\
    \bottomrule
  \end{tabular}
  \caption{Computational complexity and relative training time compared to Baseline MLP.}
  \label{tab:efficiency}
\end{table}

ASAN introduces a modest parameter overhead through $\theta$ (5 scalars) and a single quadratic pooling step. The forward pass computational complexity is $\mathcal{O}(N)$ for the alignment operation $X_c R(\theta)$, and approximately $\mathcal{O}(N^2)$ for the quadratic expansion (bounded by $N \leq 15$), followed by the MLP’s $\mathcal{O}(1)$ cost due to fixed layer sizes. In practice, training time and peak memory usage align closely with the baseline MLP at 1.2x, contrasting sharply with the higher costs of equivariant graph models at 3.0x.

\section{Limitations}
ASAN assumes a fixed atom budget of $N=15$, limiting its applicability to larger molecules without dynamic padding or masking. It currently lacks explicit encoding of chemical types or bond graphs, which could be addressed by extending the pooling layer with learned atom embeddings and adjacency matrices. Furthermore, the learned alignment, while robust, does not constitute a formal $SE(3)$-equivariant operator; for tasks requiring strict equivariance, other models may remain necessary.

\section{Conclusion}
I have presented the Adaptive Symmetry-Aware Network (ASAN), a compact regression model that learns to canonicalize 3D molecular coordinates and pool geometric features for property prediction. On a 1,000-sample subset of the QM9 dataset, ASAN achieves an MAE of 0.0460 eV, surpassing a coordinate MLP (0.0527 eV) and narrowly outperforming a fixed-invariant model (0.0468 eV), while retaining the computational simplicity of an MLP. This efficiency-accuracy trade-off positions ASAN as a practical choice for small datasets, rapid iteration, and resource-constrained environments. Future work will focus on scaling to the full QM9 dataset, incorporating atom and bond features through enriched pooling mechanisms, and exploring hybrid architectures that combine ASAN’s learned canonicalization with lightweight equivariant layers.

\paragraph{Reproducibility.} The experimental setup, including code layout, training scripts, and instructions, mirrors the documentation of the accompanying repository, covering data generation, 5-fold cross-validation, evaluation, and analysis.

\end{document}